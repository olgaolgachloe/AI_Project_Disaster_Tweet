{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitterclassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHUAHHQ-cRcF",
        "outputId": "425b6845-6345-45d2-b0ba-5c3754a9a36e"
      },
      "source": [
        "!pip install -q wandb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 47.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 180 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "Bba-Ac_ucVQ-",
        "outputId": "51dc39ea-ac5f-47b5-c40b-eb0e0fb43bd7"
      },
      "source": [
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/gyin/uncategorized/runs/fhbvqdv8\" target=\"_blank\">generous-armadillo-4</a></strong> to <a href=\"https://wandb.ai/gyin/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f6130c7f5d0>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/gyin/uncategorized/runs/fhbvqdv8?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak5uIWK9MHQg",
        "outputId": "3a4124db-34fe-46bf-f294-015514f1d4f8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joined_test.csv   run_twitter_classification.py  wandb\n",
            "joined_train.csv  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFBQidelUThZ",
        "outputId": "e2f3a359-03b4-4d37-8cb5-51777e08c6c9"
      },
      "source": [
        "!head ./joined_train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text,label\n",
            "cause our ego is crushed at that point,0\n",
            "legionnaires disease whats being done to stop deadly outbreak the worstever outbreak of legio  losangeles,1\n",
            "schools should brace for five years of upheaval from a triumphant party with gove at its heart,0\n",
            "bitches be takin pics with bags bigger than they whole body,0\n",
            "sinking into her brothers side she pouts it isnt my fault you got drowned before i got my,0\n",
            "dont blink  wont see the lightning take the w,1\n",
            "i thought it was the 10 of swords then i googled it as well but it makes more sense to be swords,0\n",
            "do you know the emergency plan at your workplace if not ask your supervisor or operations manager retail,0\n",
            "i tell my cousins i dont wanna hang out and they text me saying were coming over honestly do you have a death wish,0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlr5aoWVUW2N",
        "outputId": "341d2056-c4b7-4e97-fa35-ac5e84f72add"
      },
      "source": [
        "!pip install -q transformers==4.12.5 datasets==1.16.1 torch==1.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 298 kB 46.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 494 kB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 45.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 42.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 40.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 48.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 55.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW7a46jsUdC9"
      },
      "source": [
        "!pip install -q torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUUnEonzVsn8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd4ea07-9835-46f6-ec0b-e9f71a116f5e"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MifXVSRaIFV",
        "outputId": "3623dca7-5957-475e-c5aa-a2b4d2bce7f2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Dec  4 04:05:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    28W / 149W |      3MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgtF9Ej3g9W2",
        "outputId": "3b43692d-0250-44a8-da60-f3c772a0a133"
      },
      "source": [
        "!pip install -q emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 26.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 30 kB 12.0 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 40 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 61 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 81 kB 6.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 170 kB 5.3 MB/s \n",
            "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBMs9bIXm88U",
        "outputId": "5faa6f51-a8ba-42b9-e62f-b7d2607860ff"
      },
      "source": [
        "# twitter bert: fine tune pretrained bert layers and fine tune head classification layer \n",
        "!python3 run_twitter_classification.py \\\n",
        "  --model_name_or_path vinai/bertweet-base \\\n",
        "  --train_file ./joined_train.csv  \\\n",
        "  --validation_file ./joined_test.csv \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --per_device_eval_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir ./model/ \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --dataloader_drop_last \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 5 \\\n",
        "  --pad_to_max_length False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/04/2021 04:35:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/04/2021 04:35:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./model/runs/Dec04_04-35-53_038a8b4f27d1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=./model/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./model/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/04/2021 04:35:53 - INFO - __main__ - load a local file for train: ./joined_train.csv\n",
            "12/04/2021 04:35:53 - INFO - __main__ - load a local file for validation: ./joined_test.csv\n",
            "12/04/2021 04:35:53 - WARNING - datasets.builder - Using custom data configuration default-3878ade78b500070\n",
            "12/04/2021 04:35:53 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/04/2021 04:35:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a\n",
            "12/04/2021 04:35:53 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "12/04/2021 04:35:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a\n",
            "100% 2/2 [00:00<00:00, 833.94it/s]\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:35:53,892 >> loading configuration file https://huggingface.co/vinai/bertweet-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:35:53,893 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:341] 2021-12-04 04:35:54,032 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:35:54,173 >> loading configuration file https://huggingface.co/vinai/bertweet-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:35:54,174 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,116 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/9a877d0d57efbfeae96fec396a35595dc8c4685fe2b7b2049c6c094e24a0e8bf.f8a4dfe5c3c45a26f9df849d732decb191dc0c05ab270799695430332d143982\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,116 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/bpe.codes from cache at /root/.cache/huggingface/transformers/1c2d05a06ac61a063ad62a7590731a28cc62f58e2802c76b5f993165f25894a9.75877d86011e5d5d46614d3a21757b705e9d20ed45a019805d25159b4837b0a4\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,116 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,116 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,116 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:35:55,117 >> loading file https://huggingface.co/vinai/bertweet-base/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:35:55,249 >> loading configuration file https://huggingface.co/vinai/bertweet-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/356366feedcea0917e30f7f235e1e062ffc2d28138445d5672a184be756c8686.a2b6026e688d1b19cebc0981d8f3a5b1668eabfda55b2c42049d5eac0bc8cb2d\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:35:55,250 >> Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 130,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"tokenizer_class\": \"BertweetTokenizer\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 64001\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils.py:414] 2021-12-04 04:35:55,412 >> Adding <mask> to the vocabulary\n",
            "[WARNING|tokenization_utils_base.py:1934] 2021-12-04 04:35:55,412 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|modeling_utils.py:1351] 2021-12-04 04:35:55,573 >> loading weights file https://huggingface.co/vinai/bertweet-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/4e07e2989cb95a6f63c704a7170b48e6e663cc203c05db424e47f4d75562cf0e.7b2adda243ecb4b085eb2d22ef1b2cd12a882a43bbb13a34c11e10f960b9bfc3\n",
            "[WARNING|modeling_utils.py:1610] 2021-12-04 04:35:59,161 >> Some weights of the model checkpoint at vinai/bertweet-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1621] 2021-12-04 04:35:59,162 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "12/04/2021 04:36:00 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-e46593e7b11a2b58.arrow\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]12/04/2021 04:36:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-3bec75626f3d981a.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:01<00:00,  3.64ba/s]\n",
            "12/04/2021 04:36:03 - INFO - __main__ - Sample 10476 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [0, 139, 43062, 41034, 19, 34821, 932, 30975, 2], 'label': 0, 'text': 'its armageddon for wheelie bins', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/04/2021 04:36:03 - INFO - __main__ - Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [0, 58, 7567, 10204, 3, 3, 3, 3, 3, 2], 'label': 0, 'text': 'he couldnt resist 🐶❤️️🎁', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/04/2021 04:36:03 - INFO - __main__ - Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [0, 74, 2486, 49353, 9997, 43884, 5446, 14422, 87, 422, 14, 649, 51229, 1336, 578, 6787, 52273, 26789, 293, 515, 59867, 853, 581, 8814, 2], 'label': 0, 'text': 'an angry agnostic groupie pushing buttons who says you cant explote kids terrorist traffickers w einstein e pste', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:541] 2021-12-04 04:36:07,658 >> The following columns in the training set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:1196] 2021-12-04 04:36:07,675 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-12-04 04:36:07,675 >>   Num examples = 13836\n",
            "[INFO|trainer.py:1198] 2021-12-04 04:36:07,675 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-12-04 04:36:07,675 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1200] 2021-12-04 04:36:07,675 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1201] 2021-12-04 04:36:07,675 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-12-04 04:36:07,675 >>   Total optimization steps = 648\n",
            "[INFO|integrations.py:501] 2021-12-04 04:36:07,689 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgyin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gyin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/gyin/huggingface/runs/35hg5gzq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20211204_043607-35hg5gzq\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "{'loss': 0.6501, 'learning_rate': 1.984567901234568e-05, 'epoch': 0.02}\n",
            "  2% 10/648 [00:09<10:23,  1.02it/s]{'loss': 0.6058, 'learning_rate': 1.969135802469136e-05, 'epoch': 0.05}\n",
            "  2% 15/648 [00:15<11:02,  1.05s/it]{'loss': 0.5391, 'learning_rate': 1.9537037037037038e-05, 'epoch': 0.07}\n",
            "  3% 20/648 [00:20<10:35,  1.01s/it]{'loss': 0.5304, 'learning_rate': 1.9382716049382716e-05, 'epoch': 0.09}\n",
            "  4% 25/648 [00:25<09:59,  1.04it/s]{'loss': 0.4824, 'learning_rate': 1.9228395061728397e-05, 'epoch': 0.12}\n",
            "{'loss': 0.4491, 'learning_rate': 1.9074074074074075e-05, 'epoch': 0.14}\n",
            "  5% 35/648 [00:35<09:54,  1.03it/s]{'loss': 0.4358, 'learning_rate': 1.8919753086419756e-05, 'epoch': 0.16}\n",
            "  6% 40/648 [00:40<09:55,  1.02it/s]{'loss': 0.4397, 'learning_rate': 1.8765432098765433e-05, 'epoch': 0.19}\n",
            "{'loss': 0.3946, 'learning_rate': 1.8611111111111114e-05, 'epoch': 0.21}\n",
            "  8% 50/648 [00:51<10:11,  1.02s/it]{'loss': 0.3762, 'learning_rate': 1.8456790123456792e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4402, 'learning_rate': 1.830246913580247e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4075, 'learning_rate': 1.814814814814815e-05, 'epoch': 0.28}\n",
            " 10% 65/648 [01:06<09:51,  1.02s/it]{'loss': 0.4011, 'learning_rate': 1.799382716049383e-05, 'epoch': 0.3}\n",
            " 11% 70/648 [01:11<09:52,  1.02s/it]{'loss': 0.4241, 'learning_rate': 1.7839506172839506e-05, 'epoch': 0.32}\n",
            " 12% 75/648 [01:16<09:36,  1.01s/it]{'loss': 0.4079, 'learning_rate': 1.7685185185185187e-05, 'epoch': 0.35}\n",
            "{'loss': 0.366, 'learning_rate': 1.7530864197530865e-05, 'epoch': 0.37}\n",
            " 13% 85/648 [01:27<10:32,  1.12s/it]{'loss': 0.3432, 'learning_rate': 1.7376543209876543e-05, 'epoch': 0.39}\n",
            "                                    {'loss': 0.4228, 'learning_rate': 1.7222222222222224e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3361, 'learning_rate': 1.70679012345679e-05, 'epoch': 0.44}\n",
            "{'loss': 0.3673, 'learning_rate': 1.6913580246913582e-05, 'epoch': 0.46}\n",
            " 16% 105/648 [01:49<09:24,  1.04s/it]{'loss': 0.3079, 'learning_rate': 1.675925925925926e-05, 'epoch': 0.49}\n",
            " 17% 110/648 [01:54<09:11,  1.03s/it]{'loss': 0.3914, 'learning_rate': 1.660493827160494e-05, 'epoch': 0.51}\n",
            " 18% 115/648 [01:59<08:43,  1.02it/s]{'loss': 0.3417, 'learning_rate': 1.645061728395062e-05, 'epoch': 0.53}\n",
            " 19% 120/648 [02:04<08:47,  1.00it/s]{'loss': 0.3745, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n",
            " 19% 125/648 [02:09<09:04,  1.04s/it]{'loss': 0.3836, 'learning_rate': 1.6141975308641978e-05, 'epoch': 0.58}\n",
            " 20% 130/648 [02:14<08:52,  1.03s/it]{'loss': 0.3303, 'learning_rate': 1.5987654320987655e-05, 'epoch': 0.6}\n",
            " 21% 135/648 [02:19<09:14,  1.08s/it]{'loss': 0.3704, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.62}\n",
            " 22% 140/648 [02:24<08:38,  1.02s/it]{'loss': 0.3415, 'learning_rate': 1.5679012345679014e-05, 'epoch': 0.65}\n",
            " 22% 145/648 [02:29<08:25,  1.01s/it]{'loss': 0.3442, 'learning_rate': 1.5524691358024692e-05, 'epoch': 0.67}\n",
            " 23% 150/648 [02:35<08:44,  1.05s/it]{'loss': 0.3309, 'learning_rate': 1.537037037037037e-05, 'epoch': 0.69}\n",
            "{'loss': 0.3817, 'learning_rate': 1.5216049382716052e-05, 'epoch': 0.72}\n",
            " 25% 160/648 [02:45<08:27,  1.04s/it]{'loss': 0.3827, 'learning_rate': 1.506172839506173e-05, 'epoch': 0.74}\n",
            " 25% 165/648 [02:50<08:05,  1.00s/it]{'loss': 0.3187, 'learning_rate': 1.4907407407407408e-05, 'epoch': 0.76}\n",
            " 26% 170/648 [02:56<09:53,  1.24s/it]{'loss': 0.3418, 'learning_rate': 1.4753086419753087e-05, 'epoch': 0.79}\n",
            " 27% 175/648 [03:02<09:12,  1.17s/it]{'loss': 0.3371, 'learning_rate': 1.4598765432098766e-05, 'epoch': 0.81}\n",
            " 28% 180/648 [03:07<08:23,  1.08s/it]{'loss': 0.3994, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n",
            " 29% 185/648 [03:13<07:58,  1.03s/it]{'loss': 0.3804, 'learning_rate': 1.4290123456790124e-05, 'epoch': 0.86}\n",
            " 29% 190/648 [03:18<08:07,  1.06s/it]{'loss': 0.3583, 'learning_rate': 1.4135802469135805e-05, 'epoch': 0.88}\n",
            " 30% 195/648 [03:23<07:51,  1.04s/it]{'loss': 0.3378, 'learning_rate': 1.3981481481481482e-05, 'epoch': 0.9}\n",
            "{'loss': 0.3331, 'learning_rate': 1.3827160493827162e-05, 'epoch': 0.93}\n",
            "{'loss': 0.3443, 'learning_rate': 1.3672839506172841e-05, 'epoch': 0.95}\n",
            " 32% 210/648 [03:39<07:38,  1.05s/it]{'loss': 0.3442, 'learning_rate': 1.351851851851852e-05, 'epoch': 0.97}\n",
            " 33% 215/648 [03:45<08:12,  1.14s/it]{'loss': 0.3858, 'learning_rate': 1.3364197530864198e-05, 'epoch': 1.0}\n",
            " 33% 216/648 [03:46<07:49,  1.09s/it][INFO|trainer.py:541] 2021-12-04 04:39:58,257 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:39:58,260 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:39:58,260 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:39:58,260 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:11,  6.29it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:17,  3.97it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.22it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:21,  3.10it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.85it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.93it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.03it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.11it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:21,  2.87it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:20,  2.94it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:21,  2.83it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:29,  1.99it/s]\u001b[A\n",
            " 19% 14/72 [00:05<00:25,  2.24it/s]\u001b[A\n",
            " 21% 15/72 [00:05<00:23,  2.39it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:21,  2.57it/s]\u001b[A\n",
            " 24% 17/72 [00:06<00:20,  2.72it/s]\u001b[A\n",
            " 25% 18/72 [00:06<00:20,  2.69it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:19,  2.74it/s]\u001b[A\n",
            " 28% 20/72 [00:07<00:17,  2.90it/s]\u001b[A\n",
            " 29% 21/72 [00:07<00:17,  2.96it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  3.02it/s]\u001b[A\n",
            " 32% 23/72 [00:08<00:15,  3.09it/s]\u001b[A\n",
            " 33% 24/72 [00:08<00:16,  2.92it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  2.98it/s]\u001b[A\n",
            " 36% 26/72 [00:09<00:16,  2.81it/s]\u001b[A\n",
            " 38% 27/72 [00:09<00:15,  2.89it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:16,  2.74it/s]\u001b[A\n",
            " 40% 29/72 [00:10<00:15,  2.84it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:14,  2.93it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:13,  3.01it/s]\u001b[A\n",
            " 44% 32/72 [00:11<00:13,  3.08it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:12,  3.12it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.11it/s]\u001b[A\n",
            " 49% 35/72 [00:12<00:11,  3.16it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:11,  3.15it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:10,  3.19it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:10,  3.24it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:10,  3.13it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  3.13it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:09,  3.12it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:10,  2.87it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:09,  2.96it/s]\u001b[A\n",
            " 61% 44/72 [00:15<00:09,  2.83it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.95it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.93it/s]\u001b[A\n",
            " 65% 47/72 [00:16<00:08,  3.03it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.09it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  3.14it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.14it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:07,  2.83it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.76it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:06,  2.86it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  2.99it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:05,  3.03it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:05,  2.90it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.97it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  3.05it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  3.07it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.92it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  3.00it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.88it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.95it/s]\u001b[A\n",
            " 89% 64/72 [00:21<00:02,  2.99it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.87it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  2.98it/s]\u001b[A\n",
            " 93% 67/72 [00:22<00:01,  3.07it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  3.08it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.93it/s]\u001b[A\n",
            " 97% 70/72 [00:23<00:00,  2.66it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.72it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.35760122537612915, 'eval_accuracy': 0.8611111044883728, 'eval_F1': 0.7101449275362318, 'eval_runtime': 25.0, 'eval_samples_per_second': 184.48, 'eval_steps_per_second': 2.92, 'epoch': 1.0}\n",
            " 33% 216/648 [04:11<07:49,  1.09s/it]\n",
            "100% 72/72 [00:24<00:00,  2.84it/s]\u001b[A\n",
            " 34% 220/648 [04:15<25:46,  3.61s/it]{'loss': 0.3458, 'learning_rate': 1.320987654320988e-05, 'epoch': 1.02}\n",
            "{'loss': 0.2408, 'learning_rate': 1.3055555555555557e-05, 'epoch': 1.04}\n",
            "{'loss': 0.318, 'learning_rate': 1.2901234567901235e-05, 'epoch': 1.06}\n",
            "{'loss': 0.2503, 'learning_rate': 1.2746913580246916e-05, 'epoch': 1.09}\n",
            "                                     {'loss': 0.2886, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n",
            "{'loss': 0.3161, 'learning_rate': 1.2438271604938273e-05, 'epoch': 1.13}\n",
            " 39% 250/648 [04:47<07:33,  1.14s/it]{'loss': 0.2658, 'learning_rate': 1.228395061728395e-05, 'epoch': 1.16}\n",
            " 39% 255/648 [04:52<06:28,  1.01it/s]{'loss': 0.3025, 'learning_rate': 1.2129629629629631e-05, 'epoch': 1.18}\n",
            " 40% 260/648 [04:57<06:34,  1.02s/it]{'loss': 0.2986, 'learning_rate': 1.1975308641975309e-05, 'epoch': 1.2}\n",
            " 41% 265/648 [05:02<06:19,  1.01it/s]{'loss': 0.3576, 'learning_rate': 1.1820987654320989e-05, 'epoch': 1.23}\n",
            "{'loss': 0.2957, 'learning_rate': 1.1666666666666668e-05, 'epoch': 1.25}\n",
            " 42% 275/648 [05:13<06:23,  1.03s/it]{'loss': 0.2838, 'learning_rate': 1.1512345679012347e-05, 'epoch': 1.27}\n",
            " 43% 280/648 [05:18<06:20,  1.03s/it]{'loss': 0.2246, 'learning_rate': 1.1358024691358025e-05, 'epoch': 1.3}\n",
            " 44% 285/648 [05:24<06:50,  1.13s/it]{'loss': 0.3835, 'learning_rate': 1.1203703703703706e-05, 'epoch': 1.32}\n",
            " 45% 290/648 [05:29<06:21,  1.06s/it]{'loss': 0.3642, 'learning_rate': 1.1049382716049384e-05, 'epoch': 1.34}\n",
            " 46% 295/648 [05:34<05:48,  1.01it/s]{'loss': 0.3844, 'learning_rate': 1.0895061728395061e-05, 'epoch': 1.37}\n",
            "{'loss': 0.2681, 'learning_rate': 1.0740740740740742e-05, 'epoch': 1.39}\n",
            " 47% 305/648 [05:46<07:16,  1.27s/it]{'loss': 0.3388, 'learning_rate': 1.058641975308642e-05, 'epoch': 1.41}\n",
            " 48% 310/648 [05:51<06:07,  1.09s/it]{'loss': 0.27, 'learning_rate': 1.04320987654321e-05, 'epoch': 1.44}\n",
            "{'loss': 0.2775, 'learning_rate': 1.0277777777777777e-05, 'epoch': 1.46}\n",
            " 49% 320/648 [06:01<05:26,  1.01it/s]{'loss': 0.3019, 'learning_rate': 1.0123456790123458e-05, 'epoch': 1.48}\n",
            "{'loss': 0.2984, 'learning_rate': 9.969135802469136e-06, 'epoch': 1.5}\n",
            "{'loss': 0.2622, 'learning_rate': 9.814814814814815e-06, 'epoch': 1.53}\n",
            " 52% 335/648 [06:17<05:10,  1.01it/s]{'loss': 0.2743, 'learning_rate': 9.660493827160495e-06, 'epoch': 1.55}\n",
            "{'loss': 0.2911, 'learning_rate': 9.506172839506174e-06, 'epoch': 1.57}\n",
            " 53% 345/648 [06:28<05:05,  1.01s/it]{'loss': 0.3056, 'learning_rate': 9.351851851851854e-06, 'epoch': 1.6}\n",
            "{'loss': 0.2438, 'learning_rate': 9.197530864197531e-06, 'epoch': 1.62}\n",
            " 55% 355/648 [06:38<04:44,  1.03it/s]{'loss': 0.2845, 'learning_rate': 9.04320987654321e-06, 'epoch': 1.64}\n",
            "{'loss': 0.2631, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.67}\n",
            "{'loss': 0.3158, 'learning_rate': 8.73456790123457e-06, 'epoch': 1.69}\n",
            " 57% 370/648 [06:54<04:44,  1.02s/it]{'loss': 0.2516, 'learning_rate': 8.580246913580249e-06, 'epoch': 1.71}\n",
            "{'loss': 0.248, 'learning_rate': 8.425925925925926e-06, 'epoch': 1.74}\n",
            " 59% 380/648 [07:04<04:47,  1.07s/it]{'loss': 0.2913, 'learning_rate': 8.271604938271606e-06, 'epoch': 1.76}\n",
            " 59% 385/648 [07:10<04:53,  1.12s/it]{'loss': 0.3358, 'learning_rate': 8.117283950617285e-06, 'epoch': 1.78}\n",
            "{'loss': 0.305, 'learning_rate': 7.962962962962963e-06, 'epoch': 1.81}\n",
            "                                     {'loss': 0.2891, 'learning_rate': 7.808641975308642e-06, 'epoch': 1.83}\n",
            " 62% 400/648 [07:26<05:02,  1.22s/it]{'loss': 0.2585, 'learning_rate': 7.654320987654322e-06, 'epoch': 1.85}\n",
            "{'loss': 0.278, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.88}\n",
            "{'loss': 0.2812, 'learning_rate': 7.34567901234568e-06, 'epoch': 1.9}\n",
            "{'loss': 0.331, 'learning_rate': 7.191358024691358e-06, 'epoch': 1.92}\n",
            " 65% 420/648 [07:47<04:00,  1.06s/it]{'loss': 0.3153, 'learning_rate': 7.0370370370370375e-06, 'epoch': 1.94}\n",
            " 66% 425/648 [07:52<03:57,  1.06s/it]{'loss': 0.2281, 'learning_rate': 6.882716049382716e-06, 'epoch': 1.97}\n",
            "{'loss': 0.3168, 'learning_rate': 6.728395061728395e-06, 'epoch': 1.99}\n",
            " 67% 432/648 [08:00<03:49,  1.06s/it][INFO|trainer.py:541] 2021-12-04 04:44:11,971 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:44:11,974 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:44:11,974 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:44:11,974 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:11,  6.29it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:17,  3.98it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.23it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:21,  3.09it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.85it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.93it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.04it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.11it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:21,  2.87it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:20,  2.95it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:21,  2.84it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:29,  2.01it/s]\u001b[A\n",
            " 19% 14/72 [00:05<00:25,  2.24it/s]\u001b[A\n",
            " 21% 15/72 [00:05<00:23,  2.40it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:21,  2.58it/s]\u001b[A\n",
            " 24% 17/72 [00:06<00:20,  2.72it/s]\u001b[A\n",
            " 25% 18/72 [00:06<00:19,  2.71it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:19,  2.75it/s]\u001b[A\n",
            " 28% 20/72 [00:07<00:17,  2.89it/s]\u001b[A\n",
            " 29% 21/72 [00:07<00:17,  2.96it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  3.02it/s]\u001b[A\n",
            " 32% 23/72 [00:08<00:15,  3.08it/s]\u001b[A\n",
            " 33% 24/72 [00:08<00:16,  2.92it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  2.98it/s]\u001b[A\n",
            " 36% 26/72 [00:09<00:16,  2.81it/s]\u001b[A\n",
            " 38% 27/72 [00:09<00:15,  2.89it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:15,  2.75it/s]\u001b[A\n",
            " 40% 29/72 [00:10<00:15,  2.86it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:14,  2.96it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:13,  3.02it/s]\u001b[A\n",
            " 44% 32/72 [00:11<00:12,  3.10it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:12,  3.12it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.11it/s]\u001b[A\n",
            " 49% 35/72 [00:12<00:11,  3.15it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:11,  3.15it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:11,  3.18it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:10,  3.21it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:10,  3.10it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  3.10it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:09,  3.11it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:10,  2.86it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:09,  2.96it/s]\u001b[A\n",
            " 61% 44/72 [00:15<00:09,  2.82it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.94it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.91it/s]\u001b[A\n",
            " 65% 47/72 [00:16<00:08,  3.01it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.08it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  3.14it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.14it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:07,  2.84it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.77it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:06,  2.86it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  2.98it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:05,  3.01it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:05,  2.89it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.95it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  3.05it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  3.08it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.93it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  3.01it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.87it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.94it/s]\u001b[A\n",
            " 89% 64/72 [00:21<00:02,  3.00it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.88it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  3.00it/s]\u001b[A\n",
            " 93% 67/72 [00:22<00:01,  3.08it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  3.09it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.93it/s]\u001b[A\n",
            " 97% 70/72 [00:23<00:00,  2.66it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.73it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.34118005633354187, 'eval_accuracy': 0.8641493320465088, 'eval_F1': 0.7400332225913621, 'eval_runtime': 25.0004, 'eval_samples_per_second': 184.477, 'eval_steps_per_second': 2.92, 'epoch': 2.0}\n",
            " 67% 432/648 [08:25<03:49,  1.06s/it]\n",
            "100% 72/72 [00:24<00:00,  2.83it/s]\u001b[A\n",
            "                                     {'loss': 0.285, 'learning_rate': 6.574074074074075e-06, 'epoch': 2.01}\n",
            " 68% 440/648 [08:33<05:50,  1.69s/it]{'loss': 0.231, 'learning_rate': 6.419753086419753e-06, 'epoch': 2.04}\n",
            " 69% 445/648 [08:38<03:48,  1.12s/it]{'loss': 0.2205, 'learning_rate': 6.265432098765433e-06, 'epoch': 2.06}\n",
            "{'loss': 0.2357, 'learning_rate': 6.111111111111112e-06, 'epoch': 2.08}\n",
            " 70% 455/648 [08:49<03:17,  1.02s/it]{'loss': 0.2735, 'learning_rate': 5.956790123456791e-06, 'epoch': 2.11}\n",
            " 71% 460/648 [08:54<03:23,  1.08s/it]{'loss': 0.23, 'learning_rate': 5.80246913580247e-06, 'epoch': 2.13}\n",
            "{'loss': 0.2571, 'learning_rate': 5.6481481481481485e-06, 'epoch': 2.15}\n",
            " 73% 470/648 [09:06<03:08,  1.06s/it]{'loss': 0.2712, 'learning_rate': 5.493827160493828e-06, 'epoch': 2.18}\n",
            " 73% 475/648 [09:11<02:48,  1.03it/s]{'loss': 0.2357, 'learning_rate': 5.339506172839507e-06, 'epoch': 2.2}\n",
            "{'loss': 0.1983, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n",
            " 75% 485/648 [09:21<02:51,  1.05s/it]{'loss': 0.2597, 'learning_rate': 5.030864197530864e-06, 'epoch': 2.25}\n",
            " 76% 490/648 [09:27<02:42,  1.03s/it]{'loss': 0.2687, 'learning_rate': 4.876543209876544e-06, 'epoch': 2.27}\n",
            "{'loss': 0.2931, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.29}\n",
            " 77% 500/648 [09:38<02:56,  1.19s/it]{'loss': 0.2447, 'learning_rate': 4.567901234567902e-06, 'epoch': 2.31}\n",
            " 77% 500/648 [09:38<02:56,  1.19s/it][INFO|trainer.py:1995] 2021-12-04 04:45:50,081 >> Saving model checkpoint to ./model/checkpoint-500\n",
            "[INFO|configuration_utils.py:417] 2021-12-04 04:45:50,083 >> Configuration saved in ./model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-04 04:45:51,901 >> Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-04 04:45:51,902 >> tokenizer config file saved in ./model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-04 04:45:51,902 >> Special tokens file saved in ./model/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2021-12-04 04:45:51,902 >> added tokens file saved in ./model/checkpoint-500/added_tokens.json\n",
            "{'loss': 0.1945, 'learning_rate': 4.413580246913581e-06, 'epoch': 2.34}\n",
            " 79% 510/648 [09:56<02:37,  1.14s/it]{'loss': 0.229, 'learning_rate': 4.2592592592592596e-06, 'epoch': 2.36}\n",
            " 79% 515/648 [10:01<02:16,  1.03s/it]{'loss': 0.266, 'learning_rate': 4.104938271604938e-06, 'epoch': 2.38}\n",
            " 80% 520/648 [10:06<02:10,  1.02s/it]{'loss': 0.2649, 'learning_rate': 3.9506172839506175e-06, 'epoch': 2.41}\n",
            " 81% 525/648 [10:11<02:06,  1.03s/it]{'loss': 0.2062, 'learning_rate': 3.796296296296297e-06, 'epoch': 2.43}\n",
            "{'loss': 0.2338, 'learning_rate': 3.641975308641976e-06, 'epoch': 2.45}\n",
            "{'loss': 0.231, 'learning_rate': 3.4876543209876544e-06, 'epoch': 2.48}\n",
            " 83% 540/648 [10:28<01:54,  1.06s/it]{'loss': 0.1997, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
            "{'loss': 0.2202, 'learning_rate': 3.1790123456790127e-06, 'epoch': 2.52}\n",
            "{'loss': 0.2658, 'learning_rate': 3.0246913580246917e-06, 'epoch': 2.55}\n",
            " 86% 555/648 [10:44<01:36,  1.03s/it]{'loss': 0.2663, 'learning_rate': 2.8703703703703706e-06, 'epoch': 2.57}\n",
            " 86% 560/648 [10:49<01:33,  1.06s/it]{'loss': 0.2542, 'learning_rate': 2.7160493827160496e-06, 'epoch': 2.59}\n",
            "                                     {'loss': 0.2441, 'learning_rate': 2.561728395061729e-06, 'epoch': 2.62}\n",
            " 88% 570/648 [10:59<01:18,  1.01s/it]{'loss': 0.2406, 'learning_rate': 2.4074074074074075e-06, 'epoch': 2.64}\n",
            " 89% 575/648 [11:05<01:13,  1.00s/it]{'loss': 0.2277, 'learning_rate': 2.2530864197530865e-06, 'epoch': 2.66}\n",
            " 90% 580/648 [11:10<01:16,  1.13s/it]{'loss': 0.2225, 'learning_rate': 2.0987654320987654e-06, 'epoch': 2.69}\n",
            "{'loss': 0.2348, 'learning_rate': 1.944444444444445e-06, 'epoch': 2.71}\n",
            "{'loss': 0.261, 'learning_rate': 1.7901234567901235e-06, 'epoch': 2.73}\n",
            " 92% 595/648 [11:25<00:52,  1.00it/s]{'loss': 0.1891, 'learning_rate': 1.6358024691358027e-06, 'epoch': 2.75}\n",
            "{'loss': 0.2766, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
            " 93% 605/648 [11:36<00:44,  1.03s/it]{'loss': 0.2406, 'learning_rate': 1.3271604938271606e-06, 'epoch': 2.8}\n",
            "                                     {'loss': 0.1763, 'learning_rate': 1.1728395061728396e-06, 'epoch': 2.82}\n",
            " 95% 615/648 [11:46<00:33,  1.01s/it]{'loss': 0.2298, 'learning_rate': 1.0185185185185185e-06, 'epoch': 2.85}\n",
            "{'loss': 0.2635, 'learning_rate': 8.641975308641976e-07, 'epoch': 2.87}\n",
            "{'loss': 0.2211, 'learning_rate': 7.098765432098766e-07, 'epoch': 2.89}\n",
            "{'loss': 0.2678, 'learning_rate': 5.555555555555555e-07, 'epoch': 2.92}\n",
            "{'loss': 0.2607, 'learning_rate': 4.012345679012346e-07, 'epoch': 2.94}\n",
            " 99% 640/648 [12:13<00:08,  1.07s/it]{'loss': 0.1994, 'learning_rate': 2.469135802469136e-07, 'epoch': 2.96}\n",
            "100% 645/648 [12:18<00:03,  1.10s/it]{'loss': 0.191, 'learning_rate': 9.259259259259259e-08, 'epoch': 2.99}\n",
            "100% 648/648 [12:21<00:00,  1.09s/it][INFO|trainer.py:541] 2021-12-04 04:48:33,374 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:48:33,376 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:48:33,376 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:48:33,376 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:11,  6.26it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:17,  3.95it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.21it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:21,  3.07it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.83it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.93it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.05it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.12it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:21,  2.89it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:20,  2.94it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:21,  2.84it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:29,  2.00it/s]\u001b[A\n",
            " 19% 14/72 [00:05<00:25,  2.24it/s]\u001b[A\n",
            " 21% 15/72 [00:05<00:23,  2.40it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:21,  2.58it/s]\u001b[A\n",
            " 24% 17/72 [00:06<00:20,  2.73it/s]\u001b[A\n",
            " 25% 18/72 [00:06<00:19,  2.70it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:19,  2.74it/s]\u001b[A\n",
            " 28% 20/72 [00:07<00:17,  2.89it/s]\u001b[A\n",
            " 29% 21/72 [00:07<00:17,  2.95it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  3.03it/s]\u001b[A\n",
            " 32% 23/72 [00:08<00:15,  3.09it/s]\u001b[A\n",
            " 33% 24/72 [00:08<00:16,  2.93it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  2.98it/s]\u001b[A\n",
            " 36% 26/72 [00:09<00:16,  2.81it/s]\u001b[A\n",
            " 38% 27/72 [00:09<00:15,  2.89it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:15,  2.75it/s]\u001b[A\n",
            " 40% 29/72 [00:10<00:15,  2.86it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:14,  2.95it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:13,  3.02it/s]\u001b[A\n",
            " 44% 32/72 [00:11<00:12,  3.08it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:12,  3.11it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.12it/s]\u001b[A\n",
            " 49% 35/72 [00:12<00:11,  3.16it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:11,  3.14it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:11,  3.17it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:10,  3.22it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:10,  3.11it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  3.12it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:09,  3.12it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:10,  2.88it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:09,  2.97it/s]\u001b[A\n",
            " 61% 44/72 [00:15<00:09,  2.83it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.94it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.91it/s]\u001b[A\n",
            " 65% 47/72 [00:16<00:08,  3.02it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.08it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  3.13it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.12it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:07,  2.83it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.77it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:06,  2.87it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  3.00it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:05,  3.03it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:05,  2.89it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.97it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  3.06it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  3.09it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.97it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  3.03it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.90it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.95it/s]\u001b[A\n",
            " 89% 64/72 [00:21<00:02,  2.99it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.87it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  2.98it/s]\u001b[A\n",
            " 93% 67/72 [00:22<00:01,  3.06it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  3.07it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.91it/s]\u001b[A\n",
            " 97% 70/72 [00:23<00:00,  2.64it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.70it/s]\u001b[A\n",
            "100% 72/72 [00:24<00:00,  2.82it/s]\u001b[A{'eval_loss': 0.36255186796188354, 'eval_accuracy': 0.8595920205116272, 'eval_F1': 0.7453758362849272, 'eval_runtime': 24.9983, 'eval_samples_per_second': 184.493, 'eval_steps_per_second': 2.92, 'epoch': 3.0}\n",
            "{'train_runtime': 770.7019, 'train_samples_per_second': 53.857, 'train_steps_per_second': 0.841, 'train_loss': 0.3088069983102657, 'epoch': 3.0}\n",
            "                                     \n",
            "100% 648/648 [12:46<00:00,  1.09s/it]\n",
            "100% 72/72 [00:24<00:00,  2.82it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1409] 2021-12-04 04:48:58,377 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 648/648 [12:46<00:00,  1.18s/it]\n",
            "[INFO|trainer.py:1995] 2021-12-04 04:48:58,382 >> Saving model checkpoint to ./model/\n",
            "[INFO|configuration_utils.py:417] 2021-12-04 04:48:58,383 >> Configuration saved in ./model/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-04 04:48:59,967 >> Model weights saved in ./model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-04 04:48:59,968 >> tokenizer config file saved in ./model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-04 04:48:59,969 >> Special tokens file saved in ./model/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2021-12-04 04:48:59,969 >> added tokens file saved in ./model/added_tokens.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.3088\n",
            "  train_runtime            = 0:12:50.70\n",
            "  train_samples            =      13836\n",
            "  train_samples_per_second =     53.857\n",
            "  train_steps_per_second   =      0.841\n",
            "[INFO|trainer.py:541] 2021-12-04 04:48:59,980 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text.\n",
            "12/04/2021 04:48:59 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:48:59,982 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:48:59,982 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:48:59,982 >>   Batch size = 64\n",
            "100% 72/72 [00:24<00:00,  2.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_F1                 =     0.7454\n",
            "  eval_accuracy           =     0.8596\n",
            "  eval_loss               =     0.3626\n",
            "  eval_runtime            = 0:00:24.93\n",
            "  eval_samples            =       4612\n",
            "  eval_samples_per_second =    184.978\n",
            "  eval_steps_per_second   =      2.928\n",
            "[INFO|modelcard.py:449] 2021-12-04 04:49:25,103 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8595920205116272}, {'name': 'F1', 'type': 'f1', 'value': 0.7453758362849272}]}\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 643... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          eval/F1 ▁▇██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/accuracy ▃█▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss ▆▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime ███▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second ▁▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second ▁▁▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss █▇▅▄▅▅▄▄▃▃▃▄▃▄▃▂▃▁▂▂▂▃▂▃▃▂▃▂▂▂▂▂▂▁▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          eval/F1 0.74538\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/accuracy 0.85959\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss 0.36255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime 24.9327\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second 184.978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second 2.928\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step 648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss 0.191\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos 769862947983360.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss 0.30881\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime 770.7019\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second 53.857\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second 0.841\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m./model/\u001b[0m: \u001b[34mhttps://wandb.ai/gyin/huggingface/runs/35hg5gzq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211204_043607-35hg5gzq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXRnoWKDgumY",
        "outputId": "e2457b35-102d-4ce2-eeb6-60de5e975dc1"
      },
      "source": [
        "# bert: fine tune pretrained bert layers and fine tune head classification layer \n",
        "!python3 run_twitter_classification.py \\\n",
        "  --model_name_or_path bert-base-uncased \\\n",
        "  --train_file ./joined_train.csv  \\\n",
        "  --validation_file ./joined_test.csv \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --max_seq_length 128 \\\n",
        "  --per_device_train_batch_size 64 \\\n",
        "  --per_device_eval_batch_size 64 \\\n",
        "  --learning_rate 2e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --output_dir ./model/ \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --dataloader_drop_last \\\n",
        "  --overwrite_output_dir \\\n",
        "  --logging_steps 5 \\\n",
        "  --pad_to_max_length False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12/04/2021 04:20:29 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "12/04/2021 04:20:29 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=True,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.EPOCH,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./model/runs/Dec04_04-20-29_038a8b4f27d1,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=./model/,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=64,\n",
            "per_device_train_batch_size=64,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./model/,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "12/04/2021 04:20:29 - INFO - __main__ - load a local file for train: ./joined_train.csv\n",
            "12/04/2021 04:20:29 - INFO - __main__ - load a local file for validation: ./joined_test.csv\n",
            "12/04/2021 04:20:29 - WARNING - datasets.builder - Using custom data configuration default-3878ade78b500070\n",
            "12/04/2021 04:20:29 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "12/04/2021 04:20:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a\n",
            "12/04/2021 04:20:29 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "12/04/2021 04:20:29 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a\n",
            "100% 2/2 [00:00<00:00, 522.23it/s]\n",
            "[INFO|file_utils.py:1753] 2021-12-04 04:20:30,056 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwsl8wy86\n",
            "Downloading: 100% 570/570 [00:00<00:00, 559kB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-04 04:20:30,185 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|file_utils.py:1765] 2021-12-04 04:20:30,185 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:20:30,186 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:20:30,186 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-04 04:20:30,315 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptda4lcc2\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 23.9kB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-04 04:20:30,448 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|file_utils.py:1765] 2021-12-04 04:20:30,448 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:20:30,578 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:20:30,579 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-04 04:20:30,848 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_zgkxihh\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 2.75MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-04 04:20:31,099 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1765] 2021-12-04 04:20:31,099 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|file_utils.py:1753] 2021-12-04 04:20:31,230 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3_pune4k\n",
            "Downloading: 100% 455k/455k [00:00<00:00, 3.42MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-04 04:20:31,500 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|file_utils.py:1765] 2021-12-04 04:20:31,501 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:20:31,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:20:31,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:20:31,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:20:31,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-12-04 04:20:31,910 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "[INFO|configuration_utils.py:588] 2021-12-04 04:20:32,048 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "[INFO|configuration_utils.py:625] 2021-12-04 04:20:32,048 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.12.5\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1753] 2021-12-04 04:20:32,349 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsbvt80gp\n",
            "Downloading: 100% 420M/420M [00:11<00:00, 38.1MB/s]\n",
            "[INFO|file_utils.py:1757] 2021-12-04 04:20:44,044 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|file_utils.py:1765] 2021-12-04 04:20:44,044 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[INFO|modeling_utils.py:1351] 2021-12-04 04:20:44,045 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "[WARNING|modeling_utils.py:1610] 2021-12-04 04:20:45,892 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:1621] 2021-12-04 04:20:45,893 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/14 [00:00<?, ?ba/s]12/04/2021 04:20:46 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-c9c87ee6286cdd4b.arrow\n",
            "Running tokenizer on dataset: 100% 14/14 [00:01<00:00, 12.71ba/s]\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]12/04/2021 04:20:47 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-3878ade78b500070/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-aadf534870bd4c44.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:00<00:00, 14.90ba/s]\n",
            "12/04/2021 04:20:47 - INFO - __main__ - Sample 10476 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 2049, 2849, 18655, 5280, 2005, 5217, 2666, 8026, 2015, 102], 'label': 0, 'text': 'its armageddon for wheelie bins', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/04/2021 04:20:47 - INFO - __main__ - Sample 1824 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 2002, 2481, 2102, 9507, 100, 102], 'label': 0, 'text': 'he couldnt resist 🐶❤️️🎁', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0]}.\n",
            "12/04/2021 04:20:47 - INFO - __main__ - Sample 409 of the training set: {'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'input_ids': [101, 2019, 4854, 12943, 28199, 2177, 2666, 6183, 11287, 2040, 2758, 2017, 2064, 2102, 4654, 24759, 12184, 4268, 9452, 4026, 11451, 1059, 15313, 1041, 8827, 2618, 102], 'label': 0, 'text': 'an angry agnostic groupie pushing buttons who says you cant explote kids terrorist traffickers w einstein e pste', 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:541] 2021-12-04 04:20:52,167 >> The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:1196] 2021-12-04 04:20:52,188 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-12-04 04:20:52,188 >>   Num examples = 13836\n",
            "[INFO|trainer.py:1198] 2021-12-04 04:20:52,188 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-12-04 04:20:52,188 >>   Instantaneous batch size per device = 64\n",
            "[INFO|trainer.py:1200] 2021-12-04 04:20:52,189 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1201] 2021-12-04 04:20:52,189 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-12-04 04:20:52,189 >>   Total optimization steps = 648\n",
            "[INFO|integrations.py:501] 2021-12-04 04:20:52,205 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgyin\u001b[0m (use `wandb login --relogin` to force relogin)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./model/\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/gyin/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/gyin/huggingface/runs/28dd2xk7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/wandb/run-20211204_042052-28dd2xk7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "{'loss': 0.675, 'learning_rate': 1.984567901234568e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6063, 'learning_rate': 1.969135802469136e-05, 'epoch': 0.05}\n",
            "  2% 15/648 [00:15<11:07,  1.06s/it]{'loss': 0.5252, 'learning_rate': 1.9537037037037038e-05, 'epoch': 0.07}\n",
            "  3% 20/648 [00:20<10:40,  1.02s/it]{'loss': 0.4855, 'learning_rate': 1.9382716049382716e-05, 'epoch': 0.09}\n",
            "{'loss': 0.4103, 'learning_rate': 1.9228395061728397e-05, 'epoch': 0.12}\n",
            "{'loss': 0.4235, 'learning_rate': 1.9074074074074075e-05, 'epoch': 0.14}\n",
            "  5% 35/648 [00:35<09:53,  1.03it/s]{'loss': 0.4434, 'learning_rate': 1.8919753086419756e-05, 'epoch': 0.16}\n",
            "{'loss': 0.3916, 'learning_rate': 1.8765432098765433e-05, 'epoch': 0.19}\n",
            "{'loss': 0.393, 'learning_rate': 1.8611111111111114e-05, 'epoch': 0.21}\n",
            "{'loss': 0.3408, 'learning_rate': 1.8456790123456792e-05, 'epoch': 0.23}\n",
            "{'loss': 0.4067, 'learning_rate': 1.830246913580247e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4049, 'learning_rate': 1.814814814814815e-05, 'epoch': 0.28}\n",
            " 10% 65/648 [01:06<09:50,  1.01s/it]{'loss': 0.3994, 'learning_rate': 1.799382716049383e-05, 'epoch': 0.3}\n",
            " 11% 70/648 [01:11<09:57,  1.03s/it]{'loss': 0.3852, 'learning_rate': 1.7839506172839506e-05, 'epoch': 0.32}\n",
            " 12% 75/648 [01:16<09:05,  1.05it/s]{'loss': 0.3937, 'learning_rate': 1.7685185185185187e-05, 'epoch': 0.35}\n",
            "{'loss': 0.3552, 'learning_rate': 1.7530864197530865e-05, 'epoch': 0.37}\n",
            "{'loss': 0.3627, 'learning_rate': 1.7376543209876543e-05, 'epoch': 0.39}\n",
            "{'loss': 0.415, 'learning_rate': 1.7222222222222224e-05, 'epoch': 0.42}\n",
            "{'loss': 0.3469, 'learning_rate': 1.70679012345679e-05, 'epoch': 0.44}\n",
            " 15% 100/648 [01:41<10:11,  1.12s/it]{'loss': 0.3491, 'learning_rate': 1.6913580246913582e-05, 'epoch': 0.46}\n",
            " 16% 105/648 [01:46<08:52,  1.02it/s]{'loss': 0.3083, 'learning_rate': 1.675925925925926e-05, 'epoch': 0.49}\n",
            " 17% 110/648 [01:51<08:48,  1.02it/s]{'loss': 0.3686, 'learning_rate': 1.660493827160494e-05, 'epoch': 0.51}\n",
            "{'loss': 0.3233, 'learning_rate': 1.645061728395062e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4066, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n",
            " 19% 125/648 [02:06<08:53,  1.02s/it]{'loss': 0.3763, 'learning_rate': 1.6141975308641978e-05, 'epoch': 0.58}\n",
            "{'loss': 0.3278, 'learning_rate': 1.5987654320987655e-05, 'epoch': 0.6}\n",
            "{'loss': 0.3413, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.62}\n",
            " 22% 140/648 [02:21<08:21,  1.01it/s]{'loss': 0.3396, 'learning_rate': 1.5679012345679014e-05, 'epoch': 0.65}\n",
            " 22% 145/648 [02:26<08:21,  1.00it/s]{'loss': 0.3502, 'learning_rate': 1.5524691358024692e-05, 'epoch': 0.67}\n",
            " 23% 150/648 [02:31<08:31,  1.03s/it]{'loss': 0.3428, 'learning_rate': 1.537037037037037e-05, 'epoch': 0.69}\n",
            " 24% 155/648 [02:36<07:53,  1.04it/s]{'loss': 0.3735, 'learning_rate': 1.5216049382716052e-05, 'epoch': 0.72}\n",
            "{'loss': 0.37, 'learning_rate': 1.506172839506173e-05, 'epoch': 0.74}\n",
            " 25% 165/648 [02:46<07:57,  1.01it/s]{'loss': 0.3209, 'learning_rate': 1.4907407407407408e-05, 'epoch': 0.76}\n",
            " 26% 170/648 [02:51<07:57,  1.00it/s]{'loss': 0.3414, 'learning_rate': 1.4753086419753087e-05, 'epoch': 0.79}\n",
            " 27% 175/648 [02:56<08:00,  1.02s/it]{'loss': 0.3489, 'learning_rate': 1.4598765432098766e-05, 'epoch': 0.81}\n",
            "{'loss': 0.4027, 'learning_rate': 1.4444444444444446e-05, 'epoch': 0.83}\n",
            " 29% 185/648 [03:06<07:42,  1.00it/s]{'loss': 0.402, 'learning_rate': 1.4290123456790124e-05, 'epoch': 0.86}\n",
            "{'loss': 0.3591, 'learning_rate': 1.4135802469135805e-05, 'epoch': 0.88}\n",
            "{'loss': 0.3505, 'learning_rate': 1.3981481481481482e-05, 'epoch': 0.9}\n",
            " 31% 200/648 [03:22<08:31,  1.14s/it]{'loss': 0.3525, 'learning_rate': 1.3827160493827162e-05, 'epoch': 0.93}\n",
            " 32% 205/648 [03:27<07:25,  1.01s/it]{'loss': 0.3297, 'learning_rate': 1.3672839506172841e-05, 'epoch': 0.95}\n",
            " 32% 210/648 [03:32<07:17,  1.00it/s]{'loss': 0.331, 'learning_rate': 1.351851851851852e-05, 'epoch': 0.97}\n",
            " 33% 215/648 [03:37<07:44,  1.07s/it]{'loss': 0.3714, 'learning_rate': 1.3364197530864198e-05, 'epoch': 1.0}\n",
            " 33% 216/648 [03:38<07:31,  1.04s/it][INFO|trainer.py:541] 2021-12-04 04:24:35,173 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:24:35,175 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:24:35,176 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:24:35,176 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:13,  5.27it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:18,  3.71it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.14it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:22,  3.04it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.77it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.90it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.01it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.10it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:19,  3.16it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:19,  3.06it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:20,  2.91it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:20,  2.82it/s]\u001b[A\n",
            " 19% 14/72 [00:04<00:20,  2.77it/s]\u001b[A\n",
            " 21% 15/72 [00:04<00:19,  2.87it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:18,  2.95it/s]\u001b[A\n",
            " 24% 17/72 [00:05<00:18,  3.00it/s]\u001b[A\n",
            " 25% 18/72 [00:05<00:18,  2.90it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:17,  2.96it/s]\u001b[A\n",
            " 28% 20/72 [00:06<00:17,  3.01it/s]\u001b[A\n",
            " 29% 21/72 [00:06<00:17,  2.90it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  2.96it/s]\u001b[A\n",
            " 32% 23/72 [00:07<00:16,  3.02it/s]\u001b[A\n",
            " 33% 24/72 [00:07<00:15,  3.06it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  3.12it/s]\u001b[A\n",
            " 36% 26/72 [00:08<00:15,  2.95it/s]\u001b[A\n",
            " 38% 27/72 [00:09<00:15,  2.85it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:15,  2.78it/s]\u001b[A\n",
            " 40% 29/72 [00:09<00:14,  2.88it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:14,  2.81it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:13,  2.93it/s]\u001b[A\n",
            " 44% 32/72 [00:10<00:14,  2.83it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:13,  2.93it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.02it/s]\u001b[A\n",
            " 49% 35/72 [00:11<00:12,  3.06it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:12,  2.92it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:11,  2.98it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:11,  3.02it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:11,  2.89it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  2.99it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:10,  3.04it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:11,  2.62it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:10,  2.76it/s]\u001b[A\n",
            " 61% 44/72 [00:14<00:10,  2.70it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.85it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.93it/s]\u001b[A\n",
            " 65% 47/72 [00:15<00:08,  3.03it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.10it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  2.96it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.00it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:08,  2.61it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.54it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:07,  2.57it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  2.73it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:06,  2.72it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:06,  2.63it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.69it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  2.81it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  2.94it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.83it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  2.91it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.83it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.91it/s]\u001b[A\n",
            " 89% 64/72 [00:22<00:02,  2.82it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.76it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  2.85it/s]\u001b[A\n",
            " 93% 67/72 [00:23<00:01,  2.93it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  2.85it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.78it/s]\u001b[A\n",
            " 97% 70/72 [00:24<00:00,  2.74it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.68it/s]\u001b[A\n",
            "                                     \n",
            " 33% 216/648 [04:04<07:31,  1.04s/it]\n",
            "100% 72/72 [00:24<00:00,  2.80it/s]\u001b[A\n",
            "                                   \u001b[A{'eval_loss': 0.3501201868057251, 'eval_accuracy': 0.8572048544883728, 'eval_F1': 0.7126637554585153, 'eval_runtime': 25.2959, 'eval_samples_per_second': 182.322, 'eval_steps_per_second': 2.886, 'epoch': 1.0}\n",
            "{'loss': 0.3383, 'learning_rate': 1.320987654320988e-05, 'epoch': 1.02}\n",
            " 35% 225/648 [04:13<09:58,  1.42s/it]{'loss': 0.246, 'learning_rate': 1.3055555555555557e-05, 'epoch': 1.04}\n",
            " 35% 230/648 [04:17<07:32,  1.08s/it]{'loss': 0.3043, 'learning_rate': 1.2901234567901235e-05, 'epoch': 1.06}\n",
            " 36% 235/648 [04:23<07:08,  1.04s/it]{'loss': 0.2349, 'learning_rate': 1.2746913580246916e-05, 'epoch': 1.09}\n",
            "{'loss': 0.3064, 'learning_rate': 1.2592592592592593e-05, 'epoch': 1.11}\n",
            " 38% 245/648 [04:33<07:15,  1.08s/it]{'loss': 0.2825, 'learning_rate': 1.2438271604938273e-05, 'epoch': 1.13}\n",
            "{'loss': 0.2456, 'learning_rate': 1.228395061728395e-05, 'epoch': 1.16}\n",
            " 39% 255/648 [04:43<06:11,  1.06it/s]{'loss': 0.2954, 'learning_rate': 1.2129629629629631e-05, 'epoch': 1.18}\n",
            " 40% 260/648 [04:48<06:12,  1.04it/s]{'loss': 0.2931, 'learning_rate': 1.1975308641975309e-05, 'epoch': 1.2}\n",
            " 41% 265/648 [04:53<06:16,  1.02it/s]{'loss': 0.3582, 'learning_rate': 1.1820987654320989e-05, 'epoch': 1.23}\n",
            "{'loss': 0.2934, 'learning_rate': 1.1666666666666668e-05, 'epoch': 1.25}\n",
            " 42% 275/648 [05:03<06:25,  1.03s/it]{'loss': 0.292, 'learning_rate': 1.1512345679012347e-05, 'epoch': 1.27}\n",
            " 43% 280/648 [05:08<06:00,  1.02it/s]{'loss': 0.2171, 'learning_rate': 1.1358024691358025e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3522, 'learning_rate': 1.1203703703703706e-05, 'epoch': 1.32}\n",
            " 45% 290/648 [05:18<06:03,  1.01s/it]{'loss': 0.3479, 'learning_rate': 1.1049382716049384e-05, 'epoch': 1.34}\n",
            " 46% 295/648 [05:23<05:41,  1.03it/s]{'loss': 0.3116, 'learning_rate': 1.0895061728395061e-05, 'epoch': 1.37}\n",
            " 46% 300/648 [05:29<06:35,  1.14s/it]{'loss': 0.2725, 'learning_rate': 1.0740740740740742e-05, 'epoch': 1.39}\n",
            "{'loss': 0.3341, 'learning_rate': 1.058641975308642e-05, 'epoch': 1.41}\n",
            " 48% 310/648 [05:39<05:49,  1.03s/it]{'loss': 0.264, 'learning_rate': 1.04320987654321e-05, 'epoch': 1.44}\n",
            "{'loss': 0.2746, 'learning_rate': 1.0277777777777777e-05, 'epoch': 1.46}\n",
            " 49% 320/648 [05:49<05:28,  1.00s/it]{'loss': 0.3036, 'learning_rate': 1.0123456790123458e-05, 'epoch': 1.48}\n",
            "{'loss': 0.2799, 'learning_rate': 9.969135802469136e-06, 'epoch': 1.5}\n",
            "                                     {'loss': 0.2291, 'learning_rate': 9.814814814814815e-06, 'epoch': 1.53}\n",
            "                                     {'loss': 0.2281, 'learning_rate': 9.660493827160495e-06, 'epoch': 1.55}\n",
            " 52% 340/648 [06:10<05:10,  1.01s/it]{'loss': 0.2793, 'learning_rate': 9.506172839506174e-06, 'epoch': 1.57}\n",
            " 53% 345/648 [06:15<05:03,  1.00s/it]{'loss': 0.3219, 'learning_rate': 9.351851851851854e-06, 'epoch': 1.6}\n",
            "{'loss': 0.2272, 'learning_rate': 9.197530864197531e-06, 'epoch': 1.62}\n",
            " 55% 355/648 [06:25<04:38,  1.05it/s]{'loss': 0.2594, 'learning_rate': 9.04320987654321e-06, 'epoch': 1.64}\n",
            " 56% 360/648 [06:30<04:45,  1.01it/s]{'loss': 0.268, 'learning_rate': 8.888888888888888e-06, 'epoch': 1.67}\n",
            " 56% 365/648 [06:35<04:34,  1.03it/s]{'loss': 0.3181, 'learning_rate': 8.73456790123457e-06, 'epoch': 1.69}\n",
            "{'loss': 0.2419, 'learning_rate': 8.580246913580249e-06, 'epoch': 1.71}\n",
            "{'loss': 0.2494, 'learning_rate': 8.425925925925926e-06, 'epoch': 1.74}\n",
            " 59% 380/648 [06:50<04:37,  1.03s/it]{'loss': 0.2452, 'learning_rate': 8.271604938271606e-06, 'epoch': 1.76}\n",
            "{'loss': 0.3173, 'learning_rate': 8.117283950617285e-06, 'epoch': 1.78}\n",
            "{'loss': 0.2616, 'learning_rate': 7.962962962962963e-06, 'epoch': 1.81}\n",
            " 61% 395/648 [07:05<04:27,  1.06s/it]{'loss': 0.2689, 'learning_rate': 7.808641975308642e-06, 'epoch': 1.83}\n",
            " 62% 400/648 [07:11<04:44,  1.15s/it]{'loss': 0.2399, 'learning_rate': 7.654320987654322e-06, 'epoch': 1.85}\n",
            " 62% 405/648 [07:16<04:18,  1.07s/it]{'loss': 0.2597, 'learning_rate': 7.500000000000001e-06, 'epoch': 1.88}\n",
            " 63% 410/648 [07:21<04:08,  1.04s/it]{'loss': 0.2674, 'learning_rate': 7.34567901234568e-06, 'epoch': 1.9}\n",
            " 64% 415/648 [07:27<04:06,  1.06s/it]{'loss': 0.3351, 'learning_rate': 7.191358024691358e-06, 'epoch': 1.92}\n",
            "{'loss': 0.3214, 'learning_rate': 7.0370370370370375e-06, 'epoch': 1.94}\n",
            "{'loss': 0.2288, 'learning_rate': 6.882716049382716e-06, 'epoch': 1.97}\n",
            " 66% 430/648 [07:42<03:37,  1.00it/s]{'loss': 0.2759, 'learning_rate': 6.728395061728395e-06, 'epoch': 1.99}\n",
            " 67% 432/648 [07:44<03:38,  1.01s/it][INFO|trainer.py:541] 2021-12-04 04:28:40,727 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:28:40,729 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:28:40,730 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:28:40,730 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:13,  5.24it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:18,  3.72it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.13it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:22,  3.03it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.75it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.89it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.00it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.07it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:19,  3.13it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:20,  3.04it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:20,  2.89it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:21,  2.81it/s]\u001b[A\n",
            " 19% 14/72 [00:04<00:20,  2.77it/s]\u001b[A\n",
            " 21% 15/72 [00:04<00:19,  2.87it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:19,  2.94it/s]\u001b[A\n",
            " 24% 17/72 [00:05<00:18,  2.98it/s]\u001b[A\n",
            " 25% 18/72 [00:05<00:18,  2.89it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:17,  2.96it/s]\u001b[A\n",
            " 28% 20/72 [00:06<00:17,  3.01it/s]\u001b[A\n",
            " 29% 21/72 [00:07<00:17,  2.91it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  2.98it/s]\u001b[A\n",
            " 32% 23/72 [00:07<00:16,  3.02it/s]\u001b[A\n",
            " 33% 24/72 [00:07<00:15,  3.05it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  3.12it/s]\u001b[A\n",
            " 36% 26/72 [00:08<00:15,  2.94it/s]\u001b[A\n",
            " 38% 27/72 [00:09<00:15,  2.87it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:15,  2.80it/s]\u001b[A\n",
            " 40% 29/72 [00:09<00:14,  2.90it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:14,  2.80it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:14,  2.92it/s]\u001b[A\n",
            " 44% 32/72 [00:10<00:14,  2.82it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:13,  2.92it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.02it/s]\u001b[A\n",
            " 49% 35/72 [00:11<00:12,  3.05it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:12,  2.90it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:11,  2.96it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:11,  3.00it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:11,  2.87it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  2.97it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:10,  3.01it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:11,  2.62it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:10,  2.76it/s]\u001b[A\n",
            " 61% 44/72 [00:14<00:10,  2.69it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.84it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.92it/s]\u001b[A\n",
            " 65% 47/72 [00:15<00:08,  3.01it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.08it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  2.95it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.01it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:07,  2.63it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.56it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:07,  2.57it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  2.73it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:06,  2.71it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:06,  2.63it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.69it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  2.81it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  2.93it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.84it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  2.93it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.83it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.91it/s]\u001b[A\n",
            " 89% 64/72 [00:22<00:02,  2.83it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.75it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  2.86it/s]\u001b[A\n",
            " 93% 67/72 [00:23<00:01,  2.93it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  2.85it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.78it/s]\u001b[A\n",
            " 97% 70/72 [00:24<00:00,  2.74it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.68it/s]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.34294742345809937, 'eval_accuracy': 0.8630642294883728, 'eval_F1': 0.7436001625355547, 'eval_runtime': 25.3312, 'eval_samples_per_second': 182.068, 'eval_steps_per_second': 2.882, 'epoch': 2.0}\n",
            " 67% 432/648 [08:09<03:38,  1.01s/it]\n",
            "100% 72/72 [00:24<00:00,  2.81it/s]\u001b[A\n",
            " 67% 435/648 [08:12<16:50,  4.75s/it]{'loss': 0.2911, 'learning_rate': 6.574074074074075e-06, 'epoch': 2.01}\n",
            "                                     {'loss': 0.185, 'learning_rate': 6.419753086419753e-06, 'epoch': 2.04}\n",
            " 69% 445/648 [08:22<03:41,  1.09s/it]{'loss': 0.2088, 'learning_rate': 6.265432098765433e-06, 'epoch': 2.06}\n",
            "{'loss': 0.2188, 'learning_rate': 6.111111111111112e-06, 'epoch': 2.08}\n",
            " 70% 455/648 [08:32<03:11,  1.01it/s]{'loss': 0.249, 'learning_rate': 5.956790123456791e-06, 'epoch': 2.11}\n",
            "{'loss': 0.2225, 'learning_rate': 5.80246913580247e-06, 'epoch': 2.13}\n",
            "{'loss': 0.2694, 'learning_rate': 5.6481481481481485e-06, 'epoch': 2.15}\n",
            "{'loss': 0.2646, 'learning_rate': 5.493827160493828e-06, 'epoch': 2.18}\n",
            "                                     {'loss': 0.2134, 'learning_rate': 5.339506172839507e-06, 'epoch': 2.2}\n",
            " 74% 480/648 [08:59<02:52,  1.03s/it]{'loss': 0.197, 'learning_rate': 5.185185185185185e-06, 'epoch': 2.22}\n",
            "{'loss': 0.2548, 'learning_rate': 5.030864197530864e-06, 'epoch': 2.25}\n",
            " 76% 490/648 [09:10<02:44,  1.04s/it]{'loss': 0.2732, 'learning_rate': 4.876543209876544e-06, 'epoch': 2.27}\n",
            "                                     {'loss': 0.2432, 'learning_rate': 4.722222222222222e-06, 'epoch': 2.29}\n",
            " 77% 500/648 [09:20<02:46,  1.12s/it]{'loss': 0.2194, 'learning_rate': 4.567901234567902e-06, 'epoch': 2.31}\n",
            " 77% 500/648 [09:20<02:46,  1.12s/it][INFO|trainer.py:1995] 2021-12-04 04:30:17,258 >> Saving model checkpoint to ./model/checkpoint-500\n",
            "[INFO|configuration_utils.py:417] 2021-12-04 04:30:17,260 >> Configuration saved in ./model/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-04 04:30:18,551 >> Model weights saved in ./model/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-04 04:30:18,552 >> tokenizer config file saved in ./model/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-04 04:30:18,553 >> Special tokens file saved in ./model/checkpoint-500/special_tokens_map.json\n",
            " 78% 505/648 [09:30<03:19,  1.39s/it]{'loss': 0.1906, 'learning_rate': 4.413580246913581e-06, 'epoch': 2.34}\n",
            " 79% 510/648 [09:35<02:24,  1.04s/it]{'loss': 0.2095, 'learning_rate': 4.2592592592592596e-06, 'epoch': 2.36}\n",
            " 79% 515/648 [09:40<02:14,  1.01s/it]{'loss': 0.2306, 'learning_rate': 4.104938271604938e-06, 'epoch': 2.38}\n",
            " 80% 520/648 [09:45<02:08,  1.01s/it]{'loss': 0.2466, 'learning_rate': 3.9506172839506175e-06, 'epoch': 2.41}\n",
            "{'loss': 0.1723, 'learning_rate': 3.796296296296297e-06, 'epoch': 2.43}\n",
            " 82% 530/648 [09:55<01:55,  1.02it/s]{'loss': 0.2235, 'learning_rate': 3.641975308641976e-06, 'epoch': 2.45}\n",
            "{'loss': 0.2132, 'learning_rate': 3.4876543209876544e-06, 'epoch': 2.48}\n",
            " 83% 540/648 [10:06<01:53,  1.05s/it]{'loss': 0.1989, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
            " 84% 545/648 [10:11<01:42,  1.01it/s]{'loss': 0.2066, 'learning_rate': 3.1790123456790127e-06, 'epoch': 2.52}\n",
            " 85% 550/648 [10:16<01:39,  1.01s/it]{'loss': 0.2464, 'learning_rate': 3.0246913580246917e-06, 'epoch': 2.55}\n",
            " 86% 555/648 [10:21<01:29,  1.03it/s]{'loss': 0.2647, 'learning_rate': 2.8703703703703706e-06, 'epoch': 2.57}\n",
            "{'loss': 0.2218, 'learning_rate': 2.7160493827160496e-06, 'epoch': 2.59}\n",
            " 87% 565/648 [10:31<01:21,  1.02it/s]{'loss': 0.2228, 'learning_rate': 2.561728395061729e-06, 'epoch': 2.62}\n",
            " 88% 570/648 [10:36<01:16,  1.02it/s]{'loss': 0.2176, 'learning_rate': 2.4074074074074075e-06, 'epoch': 2.64}\n",
            " 89% 575/648 [10:41<01:12,  1.01it/s]{'loss': 0.2023, 'learning_rate': 2.2530864197530865e-06, 'epoch': 2.66}\n",
            " 90% 580/648 [10:46<01:08,  1.00s/it]{'loss': 0.2034, 'learning_rate': 2.0987654320987654e-06, 'epoch': 2.69}\n",
            "{'loss': 0.204, 'learning_rate': 1.944444444444445e-06, 'epoch': 2.71}\n",
            " 91% 590/648 [10:56<00:59,  1.02s/it]{'loss': 0.1883, 'learning_rate': 1.7901234567901235e-06, 'epoch': 2.73}\n",
            " 92% 595/648 [11:01<00:54,  1.02s/it]{'loss': 0.1861, 'learning_rate': 1.6358024691358027e-06, 'epoch': 2.75}\n",
            "{'loss': 0.2245, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
            " 93% 605/648 [11:11<00:42,  1.01it/s]{'loss': 0.2104, 'learning_rate': 1.3271604938271606e-06, 'epoch': 2.8}\n",
            " 94% 610/648 [11:16<00:37,  1.00it/s]{'loss': 0.1713, 'learning_rate': 1.1728395061728396e-06, 'epoch': 2.82}\n",
            " 95% 615/648 [11:21<00:34,  1.03s/it]{'loss': 0.2092, 'learning_rate': 1.0185185185185185e-06, 'epoch': 2.85}\n",
            " 96% 620/648 [11:26<00:28,  1.01s/it]{'loss': 0.2661, 'learning_rate': 8.641975308641976e-07, 'epoch': 2.87}\n",
            "{'loss': 0.2051, 'learning_rate': 7.098765432098766e-07, 'epoch': 2.89}\n",
            " 97% 630/648 [11:36<00:18,  1.01s/it]{'loss': 0.2323, 'learning_rate': 5.555555555555555e-07, 'epoch': 2.92}\n",
            "{'loss': 0.2265, 'learning_rate': 4.012345679012346e-07, 'epoch': 2.94}\n",
            " 99% 640/648 [11:46<00:08,  1.04s/it]{'loss': 0.1867, 'learning_rate': 2.469135802469136e-07, 'epoch': 2.96}\n",
            "{'loss': 0.1957, 'learning_rate': 9.259259259259259e-08, 'epoch': 2.99}\n",
            "100% 648/648 [11:54<00:00,  1.09s/it][INFO|trainer.py:541] 2021-12-04 04:32:51,197 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:32:51,200 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:32:51,200 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:32:51,200 >>   Batch size = 64\n",
            "\n",
            "  0% 0/72 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/72 [00:00<00:13,  5.24it/s]\u001b[A\n",
            "  4% 3/72 [00:00<00:18,  3.78it/s]\u001b[A\n",
            "  6% 4/72 [00:01<00:21,  3.15it/s]\u001b[A\n",
            "  7% 5/72 [00:01<00:21,  3.07it/s]\u001b[A\n",
            "  8% 6/72 [00:01<00:23,  2.79it/s]\u001b[A\n",
            " 10% 7/72 [00:02<00:22,  2.94it/s]\u001b[A\n",
            " 11% 8/72 [00:02<00:21,  3.03it/s]\u001b[A\n",
            " 12% 9/72 [00:02<00:20,  3.11it/s]\u001b[A\n",
            " 14% 10/72 [00:03<00:19,  3.16it/s]\u001b[A\n",
            " 15% 11/72 [00:03<00:19,  3.08it/s]\u001b[A\n",
            " 17% 12/72 [00:03<00:20,  2.92it/s]\u001b[A\n",
            " 18% 13/72 [00:04<00:20,  2.82it/s]\u001b[A\n",
            " 19% 14/72 [00:04<00:20,  2.78it/s]\u001b[A\n",
            " 21% 15/72 [00:04<00:19,  2.88it/s]\u001b[A\n",
            " 22% 16/72 [00:05<00:19,  2.94it/s]\u001b[A\n",
            " 24% 17/72 [00:05<00:18,  2.99it/s]\u001b[A\n",
            " 25% 18/72 [00:05<00:18,  2.89it/s]\u001b[A\n",
            " 26% 19/72 [00:06<00:17,  2.97it/s]\u001b[A\n",
            " 28% 20/72 [00:06<00:17,  3.02it/s]\u001b[A\n",
            " 29% 21/72 [00:06<00:17,  2.89it/s]\u001b[A\n",
            " 31% 22/72 [00:07<00:16,  2.96it/s]\u001b[A\n",
            " 32% 23/72 [00:07<00:16,  3.02it/s]\u001b[A\n",
            " 33% 24/72 [00:07<00:15,  3.04it/s]\u001b[A\n",
            " 35% 25/72 [00:08<00:15,  3.10it/s]\u001b[A\n",
            " 36% 26/72 [00:08<00:15,  2.95it/s]\u001b[A\n",
            " 38% 27/72 [00:08<00:15,  2.87it/s]\u001b[A\n",
            " 39% 28/72 [00:09<00:15,  2.79it/s]\u001b[A\n",
            " 40% 29/72 [00:09<00:14,  2.88it/s]\u001b[A\n",
            " 42% 30/72 [00:10<00:15,  2.79it/s]\u001b[A\n",
            " 43% 31/72 [00:10<00:14,  2.92it/s]\u001b[A\n",
            " 44% 32/72 [00:10<00:14,  2.84it/s]\u001b[A\n",
            " 46% 33/72 [00:11<00:13,  2.93it/s]\u001b[A\n",
            " 47% 34/72 [00:11<00:12,  3.03it/s]\u001b[A\n",
            " 49% 35/72 [00:11<00:12,  3.06it/s]\u001b[A\n",
            " 50% 36/72 [00:12<00:12,  2.92it/s]\u001b[A\n",
            " 51% 37/72 [00:12<00:11,  2.99it/s]\u001b[A\n",
            " 53% 38/72 [00:12<00:11,  3.03it/s]\u001b[A\n",
            " 54% 39/72 [00:13<00:11,  2.87it/s]\u001b[A\n",
            " 56% 40/72 [00:13<00:10,  2.97it/s]\u001b[A\n",
            " 57% 41/72 [00:13<00:10,  3.01it/s]\u001b[A\n",
            " 58% 42/72 [00:14<00:11,  2.62it/s]\u001b[A\n",
            " 60% 43/72 [00:14<00:10,  2.76it/s]\u001b[A\n",
            " 61% 44/72 [00:14<00:10,  2.69it/s]\u001b[A\n",
            " 62% 45/72 [00:15<00:09,  2.83it/s]\u001b[A\n",
            " 64% 46/72 [00:15<00:08,  2.91it/s]\u001b[A\n",
            " 65% 47/72 [00:15<00:08,  3.00it/s]\u001b[A\n",
            " 67% 48/72 [00:16<00:07,  3.08it/s]\u001b[A\n",
            " 68% 49/72 [00:16<00:07,  2.95it/s]\u001b[A\n",
            " 69% 50/72 [00:16<00:07,  3.00it/s]\u001b[A\n",
            " 71% 51/72 [00:17<00:08,  2.61it/s]\u001b[A\n",
            " 72% 52/72 [00:17<00:07,  2.56it/s]\u001b[A\n",
            " 74% 53/72 [00:18<00:07,  2.57it/s]\u001b[A\n",
            " 75% 54/72 [00:18<00:06,  2.74it/s]\u001b[A\n",
            " 76% 55/72 [00:18<00:06,  2.72it/s]\u001b[A\n",
            " 78% 56/72 [00:19<00:06,  2.63it/s]\u001b[A\n",
            " 79% 57/72 [00:19<00:05,  2.70it/s]\u001b[A\n",
            " 81% 58/72 [00:19<00:04,  2.82it/s]\u001b[A\n",
            " 82% 59/72 [00:20<00:04,  2.95it/s]\u001b[A\n",
            " 83% 60/72 [00:20<00:04,  2.84it/s]\u001b[A\n",
            " 85% 61/72 [00:20<00:03,  2.92it/s]\u001b[A\n",
            " 86% 62/72 [00:21<00:03,  2.85it/s]\u001b[A\n",
            " 88% 63/72 [00:21<00:03,  2.92it/s]\u001b[A\n",
            " 89% 64/72 [00:21<00:02,  2.86it/s]\u001b[A\n",
            " 90% 65/72 [00:22<00:02,  2.78it/s]\u001b[A\n",
            " 92% 66/72 [00:22<00:02,  2.87it/s]\u001b[A\n",
            " 93% 67/72 [00:23<00:01,  2.94it/s]\u001b[A\n",
            " 94% 68/72 [00:23<00:01,  2.84it/s]\u001b[A\n",
            " 96% 69/72 [00:23<00:01,  2.76it/s]\u001b[A\n",
            " 97% 70/72 [00:24<00:00,  2.74it/s]\u001b[A\n",
            " 99% 71/72 [00:24<00:00,  2.69it/s]\u001b[A\n",
            "100% 72/72 [00:24<00:00,  2.80it/s]\u001b[A{'eval_loss': 0.3828601539134979, 'eval_accuracy': 0.8559027910232544, 'eval_F1': 0.7424359968968192, 'eval_runtime': 25.2685, 'eval_samples_per_second': 182.52, 'eval_steps_per_second': 2.889, 'epoch': 3.0}\n",
            "{'train_runtime': 744.2822, 'train_samples_per_second': 55.769, 'train_steps_per_second': 0.871, 'train_loss': 0.29579383033661194, 'epoch': 3.0}\n",
            "                                     \n",
            "100% 648/648 [12:20<00:00,  1.09s/it]\n",
            "100% 72/72 [00:24<00:00,  2.80it/s]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:1409] 2021-12-04 04:33:16,471 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 648/648 [12:20<00:00,  1.14s/it]\n",
            "[INFO|trainer.py:1995] 2021-12-04 04:33:16,475 >> Saving model checkpoint to ./model/\n",
            "[INFO|configuration_utils.py:417] 2021-12-04 04:33:16,477 >> Configuration saved in ./model/config.json\n",
            "[INFO|modeling_utils.py:1069] 2021-12-04 04:33:17,762 >> Model weights saved in ./model/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2034] 2021-12-04 04:33:17,764 >> tokenizer config file saved in ./model/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2040] 2021-12-04 04:33:17,764 >> Special tokens file saved in ./model/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.2958\n",
            "  train_runtime            = 0:12:24.28\n",
            "  train_samples            =      13836\n",
            "  train_samples_per_second =     55.769\n",
            "  train_steps_per_second   =      0.871\n",
            "[INFO|trainer.py:541] 2021-12-04 04:33:17,835 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text.\n",
            "12/04/2021 04:33:17 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2243] 2021-12-04 04:33:17,837 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2245] 2021-12-04 04:33:17,837 >>   Num examples = 4612\n",
            "[INFO|trainer.py:2248] 2021-12-04 04:33:17,837 >>   Batch size = 64\n",
            "100% 72/72 [00:24<00:00,  2.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_F1                 =     0.7424\n",
            "  eval_accuracy           =     0.8559\n",
            "  eval_loss               =     0.3829\n",
            "  eval_runtime            = 0:00:25.25\n",
            "  eval_samples            =       4612\n",
            "  eval_samples_per_second =     182.62\n",
            "  eval_steps_per_second   =      2.891\n",
            "[INFO|modelcard.py:449] 2021-12-04 04:33:43,248 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.8559027910232544}, {'name': 'F1', 'type': 'f1', 'value': 0.7424359968968192}]}\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 525... (success).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          eval/F1 ▁███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/accuracy ▂█▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss ▂▁██\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime ▅█▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second ▄▁▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second ▄▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate ████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss █▆▄▄▄▅▄▅▃▄▄▅▄▄▃▂▃▂▂▂▂▃▂▂▂▂▃▁▂▂▂▁▂▁▂▂▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                          eval/F1 0.74244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    eval/accuracy 0.8559\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss 0.38286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime 25.2547\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second 182.62\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second 2.891\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step 648\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss 0.1957\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos 764008727001600.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss 0.29579\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime 744.2822\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second 55.769\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second 0.871\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33m./model/\u001b[0m: \u001b[34mhttps://wandb.ai/gyin/huggingface/runs/28dd2xk7\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20211204_042052-28dd2xk7/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xGOjRPDyLDe"
      },
      "source": [
        "!rm -rf ./model/checkpoint-500/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfVat0Qgtt9z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19af59d9-37f6-4745-d806-d46ed2b32b1c"
      },
      "source": [
        "!zip -r ./model.zip ./model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: model/ (stored 0%)\n",
            "  adding: model/trainer_state.json (deflated 85%)\n",
            "  adding: model/special_tokens_map.json (deflated 56%)\n",
            "  adding: model/config.json (deflated 52%)\n",
            "  adding: model/added_tokens.json (stored 0%)\n",
            "  adding: model/vocab.txt (deflated 51%)\n",
            "  adding: model/training_args.bin (deflated 49%)\n",
            "  adding: model/README.md (deflated 49%)\n",
            "  adding: model/pytorch_model.bin (deflated 14%)\n",
            "  adding: model/all_results.json (deflated 55%)\n",
            "  adding: model/bpe.codes (deflated 58%)\n",
            "  adding: model/eval_results.json (deflated 44%)\n",
            "  adding: model/runs/ (stored 0%)\n",
            "  adding: model/runs/Dec04_04-35-53_038a8b4f27d1/ (stored 0%)\n",
            "  adding: model/runs/Dec04_04-35-53_038a8b4f27d1/1638592567.686738/ (stored 0%)\n",
            "  adding: model/runs/Dec04_04-35-53_038a8b4f27d1/1638592567.686738/events.out.tfevents.1638592567.038a8b4f27d1.621.1 (deflated 62%)\n",
            "  adding: model/runs/Dec04_04-35-53_038a8b4f27d1/events.out.tfevents.1638592567.038a8b4f27d1.621.0 (deflated 66%)\n",
            "  adding: model/runs/Dec04_04-35-53_038a8b4f27d1/events.out.tfevents.1638593364.038a8b4f27d1.621.2 (deflated 28%)\n",
            "  adding: model/train_results.json (deflated 40%)\n",
            "  adding: model/tokenizer_config.json (deflated 50%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "c1XenDB-yQrS",
        "outputId": "14109a76-b0ed-4e60-c672-28e92d69d5ed"
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"./model.zip\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_bb764f88-7d6f-4b5a-b823-b82754e7f6ad\", \"model.zip\", 463399386)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBrKcjh_ygFn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}